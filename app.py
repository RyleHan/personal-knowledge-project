import streamlit as st
import requests
import json
import os
import pandas as pd
from datetime import datetime
import streamlit.components.v1 as components
import networkx as nx
from pyvis.network import Network
import tempfile
import matplotlib.pyplot as plt
import matplotlib
import numpy as np

# æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ å…¼å®¹æ€§è¡¥ä¸
if not hasattr(np, 'alltrue') and hasattr(np, 'all'):
    np.alltrue = np.all
    print("å·²åº”ç”¨ NumPy 2.0 å…¼å®¹æ€§è¡¥ä¸")

matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="ä¸ªäººçŸ¥è¯†åº“å¢å¼ºå‰‚",
    page_icon="ğŸ§ ",
    layout="wide",
)

# APIç«¯ç‚¹
API_URL = "http://localhost:8000"

# è®¾ç½®APIå¯†é’¥
if "api_key_set" not in st.session_state:
    st.session_state.api_key_set = False

# æ ‡é¢˜å’Œä»‹ç»
st.title("ğŸ§  ä¸ªäººçŸ¥è¯†åº“å¢å¼ºå‰‚")
st.markdown("""
è¿™ä¸ªå·¥å…·å¯ä»¥å¸®ä½ :
- ğŸ“š ç®¡ç†å’Œç»„ç»‡ä¸ªäººæ–‡æ¡£
- ğŸ” æ™ºèƒ½æœç´¢æ–‡æ¡£å†…å®¹
- ğŸ’¡ å‘ç°æ–‡æ¡£é—´çš„å…³è”
- ğŸ“Š å¯è§†åŒ–çŸ¥è¯†ç»“æ„
""")

# ä¾§è¾¹æ  - è®¾ç½®APIå¯†é’¥
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    
    api_key = st.text_input("è¾“å…¥Anthropic APIå¯†é’¥", type="password")
    if st.button("ä¿å­˜APIå¯†é’¥"):
        if api_key:
            os.environ["ANTHROPIC_API_KEY"] = api_key
            st.session_state.api_key_set = True
            st.success("APIå¯†é’¥å·²ä¿å­˜!")
        else:
            st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„APIå¯†é’¥")
    
    st.divider()
    st.markdown("### ğŸ“ˆ çŸ¥è¯†åº“ç»Ÿè®¡")
    try:
        response = requests.get(f"{API_URL}/documents")
        if response.status_code == 200:
            docs = response.json().get("documents", [])
            st.metric("æ–‡æ¡£æ€»æ•°", len(docs))
            if docs:
                total_size = sum(doc["size_kb"] for doc in docs)
                st.metric("æ€»å®¹é‡", f"{total_size:.2f} KB")
        else:
            st.warning("æ— æ³•è·å–çŸ¥è¯†åº“ç»Ÿè®¡")
    except:
        st.warning("åç«¯æœåŠ¡æœªå¯åŠ¨")

# åˆ›å»ºæ ‡ç­¾é¡µ
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ ä¸Šä¼ æ–‡æ¡£", "ğŸ” æœç´¢çŸ¥è¯†", "ğŸ“š çŸ¥è¯†åº“ç®¡ç†", "ğŸ”„ çŸ¥è¯†å…³è”å¯è§†åŒ–"])

# ä¸Šä¼ æ–‡æ¡£æ ‡ç­¾é¡µ
with tab1:
    st.header("ä¸Šä¼ æ–°æ–‡æ¡£")
    
    uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªæ–‡æ¡£ (PDF, DOCX, TXT)", type=["pdf", "docx", "txt"])
    
    if uploaded_file and st.button("å¤„ç†æ–‡æ¡£"):
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
            try:
                files = {"file": (uploaded_file.name, uploaded_file, "application/octet-stream")}
                response = requests.post(f"{API_URL}/upload", files=files)
                
                if response.status_code == 200:
                    result = response.json()
                    st.success(f"æ–‡æ¡£ '{result['filename']}' æˆåŠŸå¤„ç†! å¤„ç†äº† {result['chunks_processed']} ä¸ªæ–‡æœ¬å—ã€‚")
                    
                    # æ˜¾ç¤ºæå–çš„å…³é”®æ¦‚å¿µ
                    st.subheader("æ–‡æ¡£å…³é”®æ¦‚å¿µ")
                    try:
                        concepts = json.loads(result['key_concepts'])
                        for concept, description in concepts.items():
                            st.markdown(f"**{concept}**: {description}")
                    except:
                        st.text(result['key_concepts'])
                else:
                    st.error(f"å¤„ç†å¤±è´¥: {response.text}")
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")

# æœç´¢çŸ¥è¯†æ ‡ç­¾é¡µ
with tab2:
    st.header("æ™ºèƒ½æœç´¢ä½ çš„çŸ¥è¯†åº“")
    
    query = st.text_input("è¾“å…¥ä½ çš„é—®é¢˜")
    
    if query and st.button("æœç´¢"):
        with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯..."):
            try:
                response = requests.post(
                    f"{API_URL}/search",
                    data={"query": query}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    st.subheader("ğŸ¤– å›ç­”")
                    st.markdown(result["answer"])
                    
                    st.subheader("ğŸ“‘ ä¿¡æ¯æ¥æº")
                    for i, source in enumerate(result["sources"], 1):
                        st.markdown(f"{i}. **{source['title']}**")
                else:
                    st.error(f"æœç´¢å¤±è´¥: {response.text}")
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")

# çŸ¥è¯†åº“ç®¡ç†æ ‡ç­¾é¡µ
with tab3:
    st.header("ç®¡ç†ä½ çš„æ–‡æ¡£")
    
    if st.button("åˆ·æ–°æ–‡æ¡£åˆ—è¡¨"):
        st.session_state.refresh_docs = True
    
    try:
        with st.spinner("è·å–æ–‡æ¡£åˆ—è¡¨..."):
            response = requests.get(f"{API_URL}/documents")
            
            if response.status_code == 200:
                docs = response.json().get("documents", [])
                
                if docs:
                    # åˆ›å»ºæ•°æ®è¡¨æ ¼
                    df = pd.DataFrame(docs)
                    df.columns = ["æ–‡ä»¶å", "è·¯å¾„", "å¤§å°(KB)"]
                    st.dataframe(df)
                else:
                    st.info("çŸ¥è¯†åº“ä¸­è¿˜æ²¡æœ‰æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚")
            else:
                st.error("è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥")
    except Exception as e:
        st.error(f"æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡: {str(e)}")

# çŸ¥è¯†å…³è”å¯è§†åŒ–æ ‡ç­¾é¡µ
with tab4:
    st.header("çŸ¥è¯†å…³è”å¯è§†åŒ–")
    
    # æ§åˆ¶é¢æ¿
    col1, col2 = st.columns([1, 3])
    
    with col1:
        st.subheader("å¯è§†åŒ–è®¾ç½®")
        
        # è¿‡æ»¤é€‰é¡¹
        relation_types = st.multiselect(
            "å…³ç³»ç±»å‹",
            ["ç›¸ä¼¼", "å‰ç½®", "æ‰©å±•", "ç¤ºä¾‹", "å¯¹ç«‹", "åŒ…å«"],
            default=["ç›¸ä¼¼", "åŒ…å«"]
        )
        
        # å¸ƒå±€é€‰é¡¹
        layout = st.selectbox(
            "å¸ƒå±€ç®—æ³•",
            ["force", "hierarchical"],
            index=0
        )
        
        # æ¼”ç¤ºæ¨¡å¼é€‰é¡¹
        demo_mode = st.checkbox("æ¼”ç¤ºæ¨¡å¼", value=False, help="ä½¿ç”¨ç¤ºä¾‹æ•°æ®ç”Ÿæˆå›¾è°±")
        
        # æ§åˆ¶æŒ‰é’®
        refresh = st.button("åˆ·æ–°çŸ¥è¯†å›¾è°±")
    
    # ä¸»è¦å›¾è°±åŒºåŸŸ
    with col2:
        if demo_mode:
            # ç”Ÿæˆæ¼”ç¤ºæ•°æ®
            demo_nodes = [
                {"id": "doc1.pdf", "name": "äººå·¥æ™ºèƒ½å¯¼è®º", "type": "document", "size": 15},
                {"id": "doc2.pdf", "name": "æœºå™¨å­¦ä¹ åŸºç¡€", "type": "document", "size": 15},
                {"id": "doc1_AI", "name": "äººå·¥æ™ºèƒ½", "type": "concept", "size": 10},
                {"id": "doc1_ML", "name": "æœºå™¨å­¦ä¹ ", "type": "concept", "size": 10},
                {"id": "doc1_DL", "name": "æ·±åº¦å­¦ä¹ ", "type": "concept", "size": 10},
                {"id": "doc2_ML", "name": "æœºå™¨å­¦ä¹ ç®—æ³•", "type": "concept", "size": 10},
                {"id": "doc2_SVM", "name": "æ”¯æŒå‘é‡æœº", "type": "concept", "size": 10}
            ]
            
            demo_links = [
                {"source": "doc1.pdf", "target": "doc1_AI", "type": "contains", "value": 0.8},
                {"source": "doc1.pdf", "target": "doc1_ML", "type": "contains", "value": 0.7},
                {"source": "doc1.pdf", "target": "doc1_DL", "type": "contains", "value": 0.6},
                {"source": "doc2.pdf", "target": "doc2_ML", "type": "contains", "value": 0.8},
                {"source": "doc2.pdf", "target": "doc2_SVM", "type": "contains", "value": 0.7},
                {"source": "doc1.pdf", "target": "doc2.pdf", "type": "similar", "value": 0.6},
                {"source": "doc1_ML", "target": "doc2_ML", "type": "similar", "value": 0.9},
                {"source": "doc1_ML", "target": "doc2_SVM", "type": "prerequisite", "value": 0.7},
                {"source": "doc1_AI", "target": "doc1_ML", "type": "contains", "value": 0.8},
                {"source": "doc1_ML", "target": "doc1_DL", "type": "prerequisite", "value": 0.8}
            ]
            
            # ç›´æ¥åœ¨è¿™é‡Œè®¾ç½®å›¾è°±æ•°æ®å˜é‡ï¼Œé¿å…é€šè¿‡ä¼šè¯çŠ¶æ€
            graph_data = {"nodes": demo_nodes, "links": demo_links}
            
            st.write(f"æ¼”ç¤ºæ¨¡å¼å·²å¯ç”¨ - èŠ‚ç‚¹æ•°é‡: {len(demo_nodes)}, è¿æ¥æ•°é‡: {len(demo_links)}")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            st.write("èŠ‚ç‚¹æ•°æ®ç±»å‹:", type(graph_data["nodes"]))
            st.write("è¿æ¥æ•°æ®ç±»å‹:", type(graph_data["links"]))
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªèŠ‚ç‚¹å’Œè¿æ¥(å¦‚æœæœ‰)
            if graph_data["nodes"]:
                st.write("ç¤ºä¾‹èŠ‚ç‚¹:", graph_data["nodes"][0])
            if graph_data["links"]:
                st.write("ç¤ºä¾‹è¿æ¥:", graph_data["links"][0])
        elif refresh or "knowledge_graph" not in st.session_state:
            with st.spinner("æ­£åœ¨ç”ŸæˆçŸ¥è¯†å›¾è°±..."):
                try:
                    response = requests.get(f"{API_URL}/knowledge-graph")
                    
                    if response.status_code == 200:
                        graph_data = response.json()
                        st.session_state.knowledge_graph = graph_data
                        
                        # è°ƒè¯•è¾“å‡º
                        st.write(f"è·å–åˆ°çš„èŠ‚ç‚¹æ•°é‡: {len(graph_data['nodes'])}")
                        st.write(f"è·å–åˆ°çš„è¿æ¥æ•°é‡: {len(graph_data['links'])}")
                    else:
                        st.error(f"è·å–çŸ¥è¯†å›¾è°±å¤±è´¥: {response.text}")
                        st.session_state.knowledge_graph = {"nodes": [], "links": []}
                except Exception as e:
                    st.error(f"è¿æ¥åç«¯å¤±è´¥: {str(e)}")
                    st.session_state.knowledge_graph = {"nodes": [], "links": []}
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºç½‘ç»œå›¾
        if hasattr(st.session_state, "knowledge_graph"):
            graph_data = st.session_state.knowledge_graph
            
            if not graph_data["nodes"]:
                st.info("æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£æˆ–å…³ç³»æ¥æ„å»ºçŸ¥è¯†å›¾è°±ã€‚è¯·ä¸Šä¼ æ›´å¤šæ–‡æ¡£ã€‚")
            else:
                # æ˜¾ç¤ºåŸå§‹æ•°æ®
                st.write(f"åŸå§‹èŠ‚ç‚¹æ•°é‡: {len(graph_data['nodes'])}")
                st.write(f"åŸå§‹è¿æ¥æ•°é‡: {len(graph_data['links'])}")
                
                # è°ƒè¯• - æŸ¥çœ‹å…³ç³»ç±»å‹çš„å®é™…å€¼
                unique_types = set(link.get("type", "unknown") for link in graph_data["links"])
                st.write("å®é™…å…³ç³»ç±»å‹:", list(unique_types))
                
                # ç¡®ä¿å…³ç³»ç±»å‹åŒ¹é…
                # ä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…é€»è¾‘ - åŒ…å«è€Œéç²¾ç¡®åŒ¹é…
                filtered_links = []
                for link in graph_data["links"]:
                    link_type = link.get("type", "").lower()
                    # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªé€‰æ‹©çš„å…³ç³»ç±»å‹åŒ…å«åœ¨å½“å‰é“¾æ¥ç±»å‹ä¸­
                    for selected_type in relation_types:
                        if selected_type.lower() in link_type:
                            filtered_links.append(link)
                            break
                
                # å¦‚æœç­›é€‰åè¿æ¥ä¸ºç©ºï¼Œä½¿ç”¨æ‰€æœ‰è¿æ¥
                if not filtered_links:
                    st.warning("ä½¿ç”¨é€‰æ‹©çš„å…³ç³»ç±»å‹æ— æ³•ç­›é€‰å‡ºä»»ä½•è¿æ¥ï¼Œæ˜¾ç¤ºæ‰€æœ‰è¿æ¥")
                    filtered_links = graph_data["links"]
                
                # åˆ›å»ºèŠ‚ç‚¹é›†åˆ
                used_nodes = set()
                for link in filtered_links:
                    used_nodes.add(link["source"])
                    used_nodes.add(link["target"])
                
                filtered_nodes = [node for node in graph_data["nodes"] 
                                if node["id"] in used_nodes]
                
                # æ˜¾ç¤ºç­›é€‰åæ•°æ®çš„æ•°é‡
                st.write(f"ç­›é€‰åèŠ‚ç‚¹æ•°é‡: {len(filtered_nodes)}")
                st.write(f"ç­›é€‰åè¿æ¥æ•°é‡: {len(filtered_links)}")
                
                if filtered_nodes:
                    st.write("ä½¿ç”¨NetworkXå’ŒMatplotlibç”Ÿæˆå›¾è°±...")
                    
                    # åˆ›å»ºNetworkXå›¾
                    G = nx.Graph()
                    
                    # æ·»åŠ èŠ‚ç‚¹
                    for node in filtered_nodes:
                        G.add_node(node["id"], type=node["type"], name=node["name"])
                    
                    # æ·»åŠ è¾¹
                    for link in filtered_links:
                        if link["source"] in G.nodes() and link["target"] in G.nodes():
                            G.add_edge(link["source"], link["target"], 
                                      type=link.get("type", "å…³è”"),
                                      weight=link.get("value", 0.5))
                    
                    # åˆ›å»ºèŠ‚ç‚¹é¢œè‰²åˆ—è¡¨
                    node_colors = []
                    for node in G.nodes():
                        if G.nodes[node].get("type") == "document":
                            node_colors.append("#5DA5DA")  # æ–‡æ¡£èŠ‚ç‚¹è“è‰²
                        else:
                            node_colors.append("#FAA43A")  # æ¦‚å¿µèŠ‚ç‚¹æ©™è‰²
                    
                    # åˆ›å»ºèŠ‚ç‚¹å¤§å°åˆ—è¡¨
                    node_sizes = []
                    for node in G.nodes():
                        if G.nodes[node].get("type") == "document":
                            node_sizes.append(800)  # æ–‡æ¡£èŠ‚ç‚¹å¤§ä¸€äº›
                        else:
                            node_sizes.append(500)  # æ¦‚å¿µèŠ‚ç‚¹å°ä¸€äº›
                    
                    # åˆ›å»ºè¾¹é¢œè‰²åˆ—è¡¨
                    edge_colors = []
                    for u, v, d in G.edges(data=True):
                        if d.get("type") == "similar":
                            edge_colors.append("#60BD68")  # ç›¸ä¼¼å…³ç³»ç»¿è‰²
                        elif d.get("type") == "prerequisite":
                            edge_colors.append("#F17CB0")  # å‰ç½®å…³ç³»ç²‰è‰²
                        elif d.get("type") == "extension":
                            edge_colors.append("#B2912F")  # æ‰©å±•å…³ç³»æ£•è‰²
                        elif d.get("type") == "example":
                            edge_colors.append("#B276B2")  # ç¤ºä¾‹å…³ç³»ç´«è‰²
                        elif d.get("type") == "opposite":
                            edge_colors.append("#D85F5F")  # å¯¹ç«‹å…³ç³»çº¢è‰²
                        elif d.get("type") == "contains":
                            edge_colors.append("#4D4D4D")  # åŒ…å«å…³ç³»é»‘è‰²
                        else:
                            edge_colors.append("#666666")  # å…¶ä»–å…³ç³»ç°è‰²
                    
                    # è®¡ç®—è¾¹å®½åº¦
                    edge_widths = []
                    for u, v, d in G.edges(data=True):
                        weight = d.get("weight", 0.5)
                        edge_widths.append(weight * 3)
                    
                    # åˆ›å»ºå›¾å½¢
                    plt.figure(figsize=(12, 8))
                    
                    # é€‰æ‹©å¸ƒå±€
                    if layout == "hierarchical":
                        pos = nx.spring_layout(G, seed=42)
                    else:
                        pos = nx.spring_layout(G, seed=42)
                    
                    # ç»˜åˆ¶èŠ‚ç‚¹
                    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.8)
                    
                    # ç»˜åˆ¶è¾¹
                    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color=edge_colors, alpha=0.7)
                    
                    # ç»˜åˆ¶æ ‡ç­¾
                    nx.draw_networkx_labels(G, pos, font_size=10, font_family="sans-serif")
                    
                    # è°ƒæ•´å¸ƒå±€
                    plt.axis("off")
                    plt.tight_layout()
                    
                    # æ˜¾ç¤ºå›¾å½¢
                    st.pyplot(plt)
                    
                    # ä¹Ÿæ˜¾ç¤ºèŠ‚ç‚¹å’Œè¾¹çš„åˆ—è¡¨ï¼Œä½œä¸ºå¤‡ä»½
                    with st.expander("æŸ¥çœ‹èŠ‚ç‚¹å’Œå…³ç³»åˆ—è¡¨"):
                        # æ˜¾ç¤ºèŠ‚ç‚¹åˆ—è¡¨
                        st.subheader("çŸ¥è¯†èŠ‚ç‚¹è¡¨æ ¼")
                        nodes_df = pd.DataFrame(filtered_nodes if filtered_nodes else graph_data["nodes"])
                        st.dataframe(nodes_df)
                        
                        # æ˜¾ç¤ºå…³ç³»åˆ—è¡¨
                        st.subheader("çŸ¥è¯†å…³è”è¡¨æ ¼")
                        links_df = pd.DataFrame(filtered_links if filtered_links else graph_data["links"])
                        st.dataframe(links_df)
                else:
                    st.info("ç­›é€‰åæ²¡æœ‰èŠ‚ç‚¹å¯æ˜¾ç¤ºã€‚è¯·é€‰æ‹©å…¶ä»–å…³ç³»ç±»å‹æˆ–æ·»åŠ æ›´å¤šæ–‡æ¡£ã€‚")
                
                # æ·»åŠ çŸ¥è¯†å›¾è°±è§£é‡Š
                with st.expander("ğŸ“– å¦‚ä½•é˜…è¯»è¿™ä¸ªçŸ¥è¯†å›¾è°±"):
                    st.markdown("""
                    **èŠ‚ç‚¹ç±»å‹**:
                    - ğŸ”µ è“è‰²: æ–‡æ¡£
                    - ğŸŸ  æ©™è‰²: æ¦‚å¿µæˆ–æœ¯è¯­
                    
                    **è¿æ¥ç±»å‹**:
                    - ğŸŸ¢ ç»¿è‰²: ç›¸ä¼¼å…³ç³»
                    - ğŸ’— ç²‰è‰²: å‰ç½®çŸ¥è¯†
                    - ğŸŸ¤ æ£•è‰²: æ‰©å±•çŸ¥è¯†
                    - ğŸŸ£ ç´«è‰²: ç¤ºä¾‹è¯´æ˜
                    - ğŸ”´ çº¢è‰²: å¯¹ç«‹æ¦‚å¿µ
                    - âš« é»‘è‰²: åŒ…å«å…³ç³»
                    
                    **äº¤äº’æç¤º**:
                    - é¼ æ ‡æ‚¬åœåœ¨èŠ‚ç‚¹æˆ–è¿æ¥ä¸Šå¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
                    - æ‹–åŠ¨èŠ‚ç‚¹å¯è°ƒæ•´å¸ƒå±€
                    - æ»šè½®ç¼©æ”¾æŸ¥çœ‹è¯¦æƒ…
                    - åŒå‡»èŠ‚ç‚¹èšç„¦ç›¸å…³å†…å®¹
                    """)

# é¡µè„š
st.divider()
st.markdown(f"Â© {datetime.now().year} ä¸ªäººçŸ¥è¯†åº“å¢å¼ºå‰‚ | ä¸Šæ¬¡æ›´æ–°: {datetime.now().strftime('%Y-%m-%d')}")